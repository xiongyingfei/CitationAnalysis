[
  [
    "@uwaterloo.ca, x59cheng@uwaterloo.ca, cnsun@uwaterloo.ca\n†Department of Computer Science and Engineering,\nThe Hong Kong University of Science and Technology, Hong Kong, China\nEmail: yqtian@ust.hk\nAbstract—Given a list L of elements and a property ψ that\nL exhibits, ddmin is a classic test input minimization algorithm\nthat aims to automatically remove ψ-irrelevant elements from L.\nThis algorithm has been widely adopted in domains such as test\ninput minimization and software debloating. Recently, ProbDD,\na variant of ddmin, has been proposed and achieved state-\nof-the-art performance. By employing Bayesian optimization,\nProbDD estimates the probability of each element in L being\nrelevant to ψ, and statistically decides which and how many\nelements should be deleted together each time. However, the\ntheoretical probabilistic model of ProbDD is rather intricate, and\nthe underlying details for the superior performance of ProbDD\nhave not been adequately explored.\nIn this paper, we conduct the first in-depth theoretical analysis\nof ProbDD, clarifying the trends in probability and subset size\nchanges and simplifying the probability model. We complement\nthis analysis with empirical experiments, including success rate\nanalysis, ablation studies, and examinations of trade-offs and\nlimitations, to further comprehend and demystify this state-\nof-the-art algorithm. Our success rate analysis reveals how\nProbDD effectively addresses bottlenecks that slow down ddmin\nby skipping inefficient queries that attempt to delete complements\nof subsets and previously tried subsets. The ablation study\nillustrates that randomness in ProbDD has no significant impact\non efficiency. These findings provide valuable insights for future\nresearch and applications of test input minimization algorithms.\nBased on the findings above, we propose CDD, a simplified\nversion of ProbDD, reducing the complexity in both theory and\nimplementation. CDD assists in 1 validating the correctness of\nour key findings, e.g., that probabilities in ProbDD essentially\nserve as monotonically increasing counters for each element, and\n2 identifying the main factors that truly contribute to ProbDD’s\nsuperior performance. Our comprehensive evaluations across 76\nbenchmarks in test input minimization and software debloating\ndemonstrate that CDD can achieve the same performance as\nProbDD, despite being much simplified.\nIndex Terms—Program Reduction, Delta Debugging, Software\nDebloating, Test Input Minimization\nI. INTRODUCTION\nDelta Debugging [1] is a seminal family of algorithms\ndesigned for software debugging, among which ddmin stands\nout as a classic test input minimization (a.k.a., test input\nreduction) algorithm. Given a list L of elements (modeling\nthe test input) and a property ψ that L exhibits, ddmin\naims to remove elements in L that are irrelevant to ψ, such\nthat the r",
    "e minimization process\nor further reduce its size. Generally, these algorithms initiate\nby parsing I into a tree structure, such as a parse tree. They\nthen iteratively extract a list L of tree nodes from the tree\nusing heuristics and apply ddmin to L to gradually condense\nthe tree. Both manners underscore the fundamental role of\nddmin as the cornerstone of test input minimization.\nIn the past years, different variants of ddmin have been\nproposed to improve its performance [15]–[19], among which\nProbabilistic Delta Debugging (ProbDD) [16] is the state of\nthe art, with notable superiority to other algorithms [1], [15].\nWhen reducing L, ProbDD utilizes a theoretical probabilistic\nmodel based on Bayesian optimization to predict how likely\nevery element in L is essential to preserve the property ψ, by\nassigning a probability to each element. ProbDD prioritizes\ndeleting elements with lower probabilities, as such elements\ngenerally have a lower possibility of being ψ-relevant. Before\neach deletion attempt, an optimal subset of elements is deter-\nmined by maximizing the Expected Reduction Gain.1 If the\ndeletion of this subset fails to preserve ψ, the probabilistic\nmodel increases the probability assigned to each element in\nthe subset. As reported [20], aided by such a probabilistic\nmodel, ProbDD significantly outperforms ddmin by reducing\nthe execution time and the query number.2\nHowever, this probabilistic model in ProbDD is rather\nintricate, and the underlying mechanisms for its superior per-\n1In each attempt, the Expected Reduction Gain is defined as the expected\nnumber of elements removed. Higher Expected Reduction Gain is preferred,\nas it indicates an expectation to delete more elements through this attempt.\n2A query is a run of the property test ψ.\narXiv:2408.04735v3  [cs.SE]  13 Jan 2025\nformance have not been adequately studied. The original paper\nof ProbDD merely showed its performance numbers without\ndeep ablation analysis on such achievements. Specifically, the\nfollowing questions are important to the research field of test\ninput minimization, but have not been answered yet.\n1) What role do probabilities play in ProbDD, and can they\nbe simplified without impacting performance?\n2) What specific bottlenecks does ProbDD overcome to\nachieve improvement compared to ddmin?\n3) How does randomness in ProbDD contribute to the perfor-\nmance improvement?\n4) What are the potential limitations of ProbDD?\nGaining a deeper understanding of the state of the art, i.e.,\nProbDD, is highly valuable for test input minimization tasks.\nBy clarifying the intrinsic reasons behind its superiority, we\ncan facilitate researchers to understand the essence of the prob-\nabilistic model, as well as its strengths and limitations. Such\ndemystification, in our view, paves the way for enlightening\nfuture research and guides users to more effectively apply\nddmin and its variants for test input minimization.\nTo this end, we conduct the first in-depth analysis of\nProbDD, starting by theoretically simplifying its probabilistic\nmodel. In the original ProbDD, probabilities are used to\ncalculate the Expected Reduction Gain, which is subsequently\nused to determine the next subset size. However, this process\nnecessitates iterative calculations, impeding the simplification\nand comprehension of ProbDD. In our study, we initially\nestablish the analytical correlation between the probability\nand subset size, allowing for probabilities and subset sizes\nto be explicitly calculated through formulas, thus eliminating\nthe need for iterative updates. Further, through mathematical\nderivation, we discover that the probability and subset size\ncan be considered nearly independent, each varying at an\napproximate ratio on their own. By theoretical prediction, the\nprobability increases approximately by a factor of\n1\n1−e−1 (≈\n1.582), and the subset size can be deduced by this probability,\nthus providing the potential for simplifying ProbDD.\nBuilding upon our theoretical analysis, we conducted ex-\ntensive evaluations of ddmin, ProbDD, and CDD across 76\ndiverse benchmarks. The experimental results confirm the\ncorrectness of our theoretical analysis, demonstrating how\nProbDD addresses bottlenecks in ddmin by skipping ineffi-\ncient queries, reveals the impact of randomness on results, and\nhighlights the limitations of ProbDD. These findings provide\nvaluable guidance for future research and the development of\ntest input minimization algorithms.\nBased on the aforementioned analysis, we propose Counter-\nBased Delta Debugging (CDD), a simplified version of\nProbDD, to explain ProbDD’s high performance. By replacing\nprobabilities with counters, CDD eliminates the probability\ncomputations required by ProbDD, thus reducing theoretical\nand implementation complexity. Our experiments demonstrate\nthat CDD aligns with ProbDD in both effectiveness and\nefficiency, which validates our previous analysis and findings.\nKey Findings.\nThrough both theoretical analysis and em-\npirical experiments, our key findings are:\n1) Through theoretical derivation, the probabilities in ProbDD\nessentially serve as monotonically increasing counters,\nand can be simplified. This suggests that the probability\nmechanism itself may not be a critical factor in ProbDD’s\nsuperior performance.\n2) The performance bottlenecks addressed by ProbDD are\ninefficient deletion attempts on complements of subsets\nand previously tried subsets, which should be considered\nto enhance efficiency.\n3) Randomness in ProbDD has no significant impact on the\nperformance. Test input minimization is an NP-complete\nproblem, and randomness in ProbDD does not produce\nsmaller results.\n4) ProbDD is faster than ddmin, but at the cost of not guaran-\nteeing 1-minimality.3 The trade-off between effectiveness\nand efficiency is inevitable, and should be leveraged ac-\ncordingly in different scenarios.\nContributions.\nWe make the following major contributions.\n• We perform the first in-depth theoretical analysis for\nProbDD, the state-of-the-art algorithm in test input mini-\nmization tasks, and identify the latent correlation between\nthe subset size and the probability of elements.\n• We propose CDD, a much simplified version of ProbDD.\n• We evaluate ddmin, ProbDD and CDD on 76 benchmarks,\nvalidating the correctness of our theoretical analysis. Addi-\ntional experiments and statistical analysis on ProbDD further\nexplain its superior performance, reveal the effectiveness of\nrandomness, and demonstrate the limitations of ProbDD.\n• To enable future research on test input minimization, we re-\nlease the artifact publicly for replication [21]. Additionally,\nwe have integrated CDD into the Perses project, available\nat https://github.com/uw-pluverse/perses.\nPaper Organization.\nThe remainder of the paper is struc-\ntured as follows: § II introduces the symbols used in this\nstudy and detailed workflow of ddmin and ProbDD. § III\npresents our in-depth analysis on ProbDD, simplifying the\nmodel of probability and subset size. § IV describes empirical\nexperiments and results, from which additional findings are de-\nrived. § V introduces CDD, which simplifies ProbDD based on\nour earlier findings while maintaining equivalent performance.\n§ VII illustrates related work and § VIII concludes this study.\nII. PRELIMINARIES\nTo facilitate comprehension, Table I lists all the important\nsymbols used in this paper. Next, this section introduces ddmin\nand ProbDD, with the running example shown in Fig. 1.\nA. The ddmin Algorithm\nThe ddmin algorithm [1] is the first algorithm to systemati-\ncally minimize a bug-triggering input to its essence, which has\nbeen widely adopted in program reduction [12]–[14], software\ndebloating [15], [22] and test suites reduction [23], [24]. It\ntakes the following two inputs:\n3A list is considered to have 1-minimality if removing any single element\nfrom it results in the loss of its property.\nTABLE I: The symbols used in this",
    "a subset of L\npr\nthe probability of each\nelement in round r\nl1:import math, sys\nl2:input = sys.argv[1]\nl3:a = int(input)\nl4:b = math.e\nl5:c = 3\nl6:d = pow(b, a)\nl7:c = math.log(d, b)\nl8:crash(c)\n(a) Original.\nl1:import math, sys\nl2:input = sys.argv[1]\nl3:a = int(input)\nl4:b = math.e\nl5:c = 3\nl6:d = pow(b, a)\nl7:c = math.log(d, b)\nl8:crash(c)\n(b) By ddmin.\nl1:import math, sys\nl2:input = sys.argv[1]\nl3:a = int(input)\nl4:b = math.e\nl5:c = 3\nl6:d = pow(b, a)\nl7:c = math.log(d, b)\nl8:crash(c)\n(c) By ProbDD.\nFig. 1: A running example in Python. Fig. 1(a) shows the\noriginal program, represented as a list of 8 elements (l1,\nl2, · · · , l8), in which l8 (i.e., crash(c)) triggers the crash.\nFig. 1(b) and Fig. 1(c) show the minimized results by ddmin\nand ProbDD, with removed elements masked in gray. Both\nminimized programs still trigger the crash. Note that ProbDD\ncannot consistently guarantee the result in Fig. 1(c) and might\nproduce larger results, due to its inherent randomness.\n• L: a list of elements representing a bug-triggering input. For\nexample, L can be a list of bytes, characters, lines, tokens,\nor parse tree nodes extracted from the bug-triggering input.\n• ψ: a property that L has. Formally, ψ can be defined as a\npredicate that returns T if a list of elements preserves the\nproperty, F otherwise.\nand returns a minimal subset of L that still pr",
    "ining elements as the result.\nRound Number r.\nNote that we introduce a round number r\nat the second column of Table II. Within each round, the list L\nis divided into subsets of a fixed size, on which Step 1 and Step\n2 are applied. A new round begins when no further progress\ncan be made with the current subset size. This round number\nis not explicitly present in the original ddmin algorithm but\nexists implicitly. In subsequent sections, we will also use this\nconcept to introduce and simplify the ProbDD algorithm.\nTable II illustrates the step-by-step minimization process of\nddmin with the running example in Fig. 1. Initially, the input\nL is [l1, l2, · · · , l8]. The ddmin algorithm iteratively generates\nvariants by gradually decreasing the subset size from 4 to 1.\n1) Round 1 (s=4). At the beginning, ddmin splits L into two\nsubsets and generates two variants v1 and v2. However,\nneither of them preserves ψ.\n2) Round 2 (s=2). Next, ddmin continues to subdivide these\ntwo subsets into smaller ones",
    ". Finally, ddmin decreases subset size s from\n2 to 1, and generates more variants. This time, v23, which\nis the complement of the subset {l5}, preserves ψ. Hence,\nthe subset {l5} is permanently removed from L. Then for\neach of the remaining subsets {l1}, {l2}, · · · , {l8}, ddmin\nrestarts testing the complement of each subset, i.e., from\nv24 to v30. However, none of these variants preserves ψ,\nand no subset can be further divided, so ddmin terminates\nwith the variant v23 as the final result.\nB. Probabilistic Delta Debugging (ProbDD)\nWang et al. [16] proposed the state-of-the-art algorithm\nProbDD, significantly surpassing ddmin in minimizing bug-\ntriggering programs on C compilers and benchmarks in soft-\nware debloating. ProbDD employs Bayesian optimization [26]\nto model the minimization problem. ProbDD assigns a proba-\nbility to each element in L, representing its likelihood of being\nessential for preserving the property ψ. At each step during\nthe minimization process, ProbDD selects a subset of elements\nexpected to yield the highest Expected Reduction Gain, and\ntargets these elements in the subset for deletion. In this section,\nwe outline ProbDD’s workflow in Algorithm 1, paving the way\nfor a deeper understanding and analysis of ProbDD.\nInitialize (line 1).\nIn L, ProbDD assigns each element an\ninitial probability p0 on line 1, representing the prior likelihood\nthat each element cannot be removed.\nStep 1: Select elements (line 4, line 14–24).\nFirst, ProbDD\nsorts the elements in L by probability in ascending order on\nline 14, and the order of elements with the same probability is\ndetermined randomly. Then, on line 19, it calculates the subset\nto be removed in the next attempt via the proposed Expected\nReduction Gain E(s), as shown in Equation (1), with E(s)\ndenoting the expected gain obtained via removing the first\nTABLE II: Step-by-step outcomes from ddmin on the running example. In each column, a variant is generated and tested\nagainst the prope",
    ")\nr = 3 (s=1)\nl1\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl2\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl3\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl4\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl5\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl6\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl7\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nl8\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nψ\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nF\nT\nF\nF\nF\nF\nF\nF\nF\ns elements in L selected for deletion, and li.p denoting the\ncurrent probability of the i-th element in L.\nE(s) = s ×\ns\nY\ni=1\n(1 −li.p)\n(1)\nNote that ProbDD has an invariant that the subset S chosen\nfor deletion attempt is always the first s elements in L. Every\ntime, the first s∗elements are selected as the optimal subset\nS, where s∗maximizes the Expected Reduction Gain E(s),\nelaborated as Equation (2).\ns∗= arg max\ns\nE(s)\n(2)\nStep 2: Delete the Subset (line 5-9).\nIf ψ is still preserved\nafter the removal of S, ProbDD removes subset S on line 6,\ni.e., keeps only the complement of S, and proceeds to Step 1.\nIf ψ cannot be preserved after the removal, on lines 8 and 9,\nProbDD updates the probability of each element in the subset\nS via Equation (3), and resumes at Step 1. It is important to\nnote that if an element li has been individually deleted but\nfailed, its probability li.p will be set to 1, indicating that this\nelement cannot be removed and will no longer be considered\nfor deletion.\nli.p ←\nli.p\n1 −Q\nl∈S (1 −l.p)\n(3)\nStep 3: Check Termination (line 3).\nIf every element either\nhas been deleted, or possesses a probability of 1, ProbDD\nterminates. If not, it returns to Step 1.\nRound Number r.\nSimilar to the concept of rounds in\nddmin (see Table II), ProbDD also has an implicit round\nnumber r, as introduced on line 2 in Algorithm 1 and the\nsecond row of Table III. During a round, the subset size is the\nsame and every subset in L is attempted for deletion. Once\nthe probabilities of all elements have been updated, the next\nround begins (i.e., r ←r + 1 on line 11).\nTable III illustrates the step-by-step results of ProbDD.\nFollowing the study of ProbDD [16], the initial probability\np0 is set to 0.25, resulting in subsets with a size of 4 as per\nEquation (2).\n1) Round 1 (s=4). Similar to the example in the original paper\nof ProbDD [16], we assume ProbDD selects (l1, l4, l5,\nl8) to delete due to the randomness, thus resulting in the\nvariant v1. However, v1 fails to exhibit ψ, leading to the\nprobability of these selected elements being updated from\n0.25 to\n0.25\n1−(1−0.25)4 ≈0.37, based on Equation (3). Next,\nthe remaining elements with lower probability, i.e., (l2, l3,\nl6, l7), are prioritized and selected for deletion, resulting in\nv2. This time, the property test passes and these elements\nare removed.\n2) Round 2 (s=2). Given that all probabili",
    "result\nin v5 and v7, respectively, while l5 and l8 are verified as\nnon-removable, thus being returned as the final result.\nIII. DELVING DEEPER INTO PROBABILITY AND SIZE\nBeginning with this section, we will systematically present\nour findings. Each finding will be introduced by first stating\nthe result, followed by the explanation. In this section, we\ntheoretically analyze the trend of probability changes across\nrounds, and the approach to derive the optimal subset size.\nA. On the Probability in ProbDD\nFinding 1:\nThe probability assigned to each element\nincreases monotonically with the round number r, by a\nfactor of approximately 1.582. Essentially, the probability\nfor each element can be expressed as a function of r and\np0, i.e.,\npr ≈1.582r × p0\nAn Illustrative Example.\nThe running example illustrated\nin Table III leads to this finding. Observation reveals that\nafter each element has been attempted for deletion once,\ni.e., completing one round, the probabilities of all remaining\nelements are updated. The initial probability is 0.25; after v2, it\nAlgorithm 1: ProbDD(L, ψ)\nInput: L: a list to be minimized.\nInput: ψ : L →B: the property to be preserved by L.\nInput: p0: the initial probability given by the user.\nOutput: the minimized list that still exhibits the property ψ.\n// Initialize the probability of each element with p0\n1 foreach l ∈L do l.p ←p0\n/* The round number r, initially 0. r is not explicitly used\nin the original ProbDD algorithm. It is displayed for\ndemonstrating ProbDD’s implicit principles.\n*/\n2 r ←0\n3 while ∃l ∈L : l.p < 1 do\n// Select elements from L for deletion attempt.\n4\nS ←SelectSubset(L)\n// Check if removing the subset preserves the property\n5\ntemp ←L \\ S\n6\nif ψ(temp) = T then L ←temp\n7\nelse\n// Calculate the factor to update probabilities\n8\nfactor ←\n1\n1−Q\nl∈S(1−l.p)\n// Update the probabilities of elements in the subset\n9\nforeach l ∈S do l.p ←factor × l.p\n10\nif All elements’ probability have been updated then\n// Move to the next round.\n11\nr = r + 1\n12 ",
    " list of elements to be reduced.\nOutput: The subset of elements that maximizes the Expected\nReduction Gain.\n/* Sort L by ascending probability, with elements having\nthe same probability in random order.\n*/\n14\nsortedL ←RandomizeThenSort(L)\n15\nS ←∅\n16\ncurrentMaxGain ←0\n17\nforeach l ∈sortedL do\n18\ntempSubset ←S ∪{l}\n19\ngain ←|tempSubset| × Q\nl∈tempSubset(1 −l.p)\n20\nif gain > currentMaxGain then\n21\ncurrentMaxGain ←gain\n22\nS ←tempSubset\n23\nelse break\n24\nreturn S\nTABLE III: Step-by-step outcomes from ProbDD on the\nrunning example. Similar to Table II, round number, subset\nsize and the details of each variants are presented. For each\nvariant, the probability of each element is noted alongside.\nInitial\nVariants\nv1\nv2\nv3\nv4\nv5\nv6\nv7\nv8\nElement\nProb\nRound\nr = 1 (s=4)\nr = 2 (s=2)\nr = 3 (s=1)\nl1\n0.25\n0.37\n✓\n0.37\n0.61\n✓\n0.61\n✓\n0.61\n✓\n0.61\nl2\n0.25\n✓\n0.25\nl3\n0.25\n✓\n0.25\nl4\n0.25\n0.37\n✓\n0.37\n✓\n0.37\n0.61\nl5\n0.25\n0.37\n✓\n0.37\n0.61\n✓\n0.61\n✓\n0.61\n1\n✓\n1\n✓\n1\nl6\n0.25\n✓\n0.25\nl7\n0.25\n✓\n0.25\nl8\n0.25\n0.37\n✓\n0.37\n✓\n0.37\n",
    "wing v4, it increases to 0.61; and by the\nend of v8, it reaches 1. Consequently, we hypothesize that with\neach deletion attempt, the probability approximately increases\nin a predictable manner. Through appropriate simplification,\nwe can theoretically model this trend, and thereby model the\nentire progression of probability changes.\n1) Assumption for Theoretical Analysis: Besides the above\nobservation from a concrete example, theoretical analysis is\nnecessary. To refine the mathematical model of ProbDD for\neasier representation, analysis and derivation, we assume that\nthe number of elements in L is always divisible by the subset\nsize. With this assumption, the probability of each element\nwill be updated in the same manner; as a result, before and\nafter each round, the probabilities of all elements are always\nthe same, as shown in Lemma III.1. This assumption is often\napplicable in practice. For instance, in the running example in\nTable III, before each round, the probabilities associated with",
    "s in L is always divis-\nible by the subset size, then after each round, the probabilities\nof all elements will always remain the same.\nProof. We use mathematical induction to prove this lemma.\nBase Case.\nInitially, all probabilities are set to the same\nvalue. Hence, before the first round, the probabilities of all\nelements are identical.\nInductive Step.\nAssume that before a given round, the\nprobabilities of all elements are identical (induction hypoth-\nesis). After failing to delete a subset S, ProbDD updates the\nprobability of each element of S according to Equation (3).\nThis formula depends solely on two factors: the current\nprobability of each element of S, i.e., li.p, and the size of the\nsubset |S|. For li.p, by the induction hypothesis, all elements\nhave the same probability at the beginning of the round; for\n|S|, if the total number of elements in L is divisible by\nthe subset size, then every subset in the round will have\nthe same size |S|. Therefore, both factors li.p and |S| are\niden",
    "eduction, the number of elements is always divisible by\nthe subset size in each round, i.e., s=4, s=2, s=1. Therefore,\nstarting with an initial probability of 0.25, the probability of\neach elements remain identical after each round, being 0.37,\n0.61 and 1, respectively.\nWhile it is not always possible for the number of elements\nto be divisible by the subset size, the elements will still be\npartitioned as evenly as possible. However, such indivisibil-\nities make the theoretical simplification of ProbDD nearly\nimpossible. Based on our observation when running ProbDD,\nbeing slightly uneven during partitioning does not significantly\naffect probability updates. Moreover, we will demonstrate that\nthe simplified algorithm derived from this assumption has\nno significant difference from ProbDD in § V, via thorough\nexperimental evaluation.\n2) Probability vs. Subset Size Correlation: In the second\nstep, we derive the correlation between probability and subset\nsize. Based on the assumption in the previous step, the\nprobability of each element is identical and represented as\npr in round r, thus the formula of Expected Reduction Gain\nfrom Equation (1) can be simplified to\nE(s) = s × (1 −pr)s\n(4)\nGiven the probability of elements pr in the round r, sr can be\nderived through gradient-based ",
    "e 1 −pr into Equation (6), ob-\ntaining\npr+1 =\npr\n1 −(1 −pr)sr =\npr\n1 −(e−1\nsr )sr\n=\npr\n1 −e−1 ≈1.582 × pr\nEquivalently, the approximate probability after round r can\nbe derived given only p0, i.e.,\npr =\np0\n(1 −e−1)r ≈1.582r × p0\nTherefore, through empirical observations on the running\nexample, coupled with theoretical derivation and simplifica-\ntion, we have identified the pattern of probability changes w.r.t.\nthe round number r, i.e., pr =\np0\n(1−e−1)r ≈1.582r × p0.\nB. On the Size of Subsets in ProbDD\nFinding 2:\nThe size of subsets in r-th round can be\nanalytically pre-determined given only the probability of this\nround, i.e., sr = arg maxs∈N+ s ×(1−pr)s, which is either\n⌊−\n1\nln(1−pr)⌋or ⌈−\n1\nln(1−pr)⌉.\nBased on the Finding 1, the probability pr can be approxi-\nmately estimated by the current round number r via a factor.\nConsequently, we can further derive the subset size sr by\nmaximizing the Expected Reduction Gain in ProbDD.\nLemma III.3. The optimal subset size sr in round r is either\n⌊−\n1\nln(1−pr)⌋or ⌈−\n1\nln(1−pr)⌉.\nProof. The Expected Reduction Gain is determined by the\nformula E(sr) = sr × (1 −pr)sr, which increases initially\nwith sr and then decreases as sr grows further, enabling the\noptimal solution to be identified through derivative analysis.\nTherefore, we can deduce the optimal sr by solving E′(sr) =\n0. Therefore, the optimal size of subsets sr in r-th round is\n−\n1\nln(1−pr), which will be rounded to either ⌊−\n1\nln(1−pr)⌋or\n⌈−\n1\nln(1−pr)⌉. The final subset size should be chosen based on\nwhich integer results in a larger Expected Reduction Gain.\nLemma III.3 allows the subset size to be analytically pre-\ndetermined, thus providing the potential for simplification of\nProbDD and leading to the proposal of CDD (detailed in § V).\nIV. EMPIRICAL EXPERIMENTS\nIn addition to the theoretical derivation above, we conduct\nan extensive experimental evaluation on ddmin and ProbDD to\ngain deeper insights and achieve further discoveries. Specifi-\ncally, we reproduce the experiments on ddmin and ProbDD\nby Wang et al. [16], and then delve deeper into ProbDD,\nanalyzing its randomness, the bottlenecks it overcomes, and\nits 1-minimality. Furthermore, we evaluate our proposed CDD\n(which will be presented in § V), validating our previous\ntheoretical analysis. Due to limited space, we present the\nresults of both ProbDD and CDD together within this section,\nbut this section primarily focuses on discussing ProbDD, while\nthe next section will focus on CDD.\nA. Benchmarks\nTo extensively evaluate ddmin, ProbDD and CDD, we use\nthe following three benchmark suites (76 benchmarks in total),\ncovering various use scenarios of minimization algorithms.\n• BMC: 20 large bug-triggering programs in C language, each\nof which triggers a real-world compiler bug in either LLVM\nor GCC. The original size of benchmarks ranges from 4,397\ntokens to 212,259 tokens. This benchmark suite has been\nused to evaluate test input minimization work [12], [16],\n[27], [28].\n• BMDBT: source programs of 10 command-line utilities. The\noriginal size of benchmarks ranges from 34,801 tokens to\n163,296 tokens. This benchmark suite was collected by\nHeo et al. [15] and used to evaluate software debloating\ntechniques [15], [29], [30].\n• BMXML: 46 XML inputs triggering 8 unique bugs in Basex,\na widely-used XML processing tool. The original size of\nbenchmarks ranges from 19,290 tokens to 20,750 tokens.\nThis benchmark suite is generated via Xpress [31] and\ncollected by the authors of this study, as the original XML\ndataset used in ProbDD paper is not publicly available.\nB. Evaluation Metrics\nWe measure the following aspects as metrics.\nFinal Size.\nThis metric assesses the effectiveness of reduc-\ntion. When reducing a list L with a certain property ψ, a\nsmaller final list is preferred, indicating that more irrelevant\nelements have been successfully eliminated. In all benchmark\nsuites, the metric is measured by the number of tokens.\nExecution Time.\nThe execution time of a minimization\nalgorithm reflects its efficiency. A minimiza",
    "ferences are significant. In general, a p-\nvalue below 0.05 denotes a significant distinction between the\ntwo groups of data. Otherwise, the observed difference lacks\nstatistical significance.\nC. The Wrapping Frameworks\nThe ddmin algorithm and its variants usually serve as the\nfundamental algorithm. To apply them to a concrete scenario,\nan outer wrapping framework is generally needed to handle\nthe structure of the input. In our evaluation, we choose the\nsame wrapping frameworks as those used by ProbDD paper.\nFor those tree-structured bug-triggering inputs, i.e., BMC and\nBMXML, we use Picireny 21.8 [33], an implementation of\nHDD [13]. Picireny parses such inputs into trees, and then\ninvokes Picire 21.8 [25], an open-sourced Delta Debugging\nlibrary with ddmin, ProbDD and CDD implemented, to reduce\neach level of the trees. For software debloating on BMDBT,\nChisel [15] is employed, in which ddmin, ProbDD and CDD\nare integrated.\nAll experiments are conducted on a server running Ubuntu\n22.04.3 LTS, with 4 TB RAM and two Intel Xeon Gold 6348\nCPUs @ 2.60GHz. To ensure the reproducibility, we employ\ndocker images to release the source code and the configuration.\nEach benchmark is reduced using a single thread. Following\nthe ProbDD paper, we run each algorithm on each benchmark\n5 times and calculate the geometric average results.\nD. Reproduction Study of ProbDD\nTo comprehensively reproduce the results of ProbDD [16],\nwe evaluate ddmin and ProbDD using three benchmark suites,\ncontaining a total of 76 benchmarks. Following the settings of\nProbDD [16], we set the empirically estimated remaining rate\nas the initialization probability p0, specifically, 0.1 for BMC\nand BMDBT, and 2.5e-3 for BMXML. The detailed results are\nshown in Table IV and Table V.\nEfficiency and Effectiveness.\nThrough our reproduction\nstudy, we find that the performance of ProbDD aligns with the\nresults reported in the original paper, showing that ProbDD is\nsignificantly more efficient than ddmin. Across three bench-\nmark suites, ProbDD requires 27.01% less time and 52.44%\nfewer queries, with p-value being 3e-09 and 9e-14, respec-\ntively. Moreover, we assess the effectiveness by measuring\nthe sizes of the final minimized results. The effectiveness of\nddmin and ProbDD varies across each benchmark, but neither\nalgorithm consistently outperforms the other, as substantiated\nby a p-value of 0.32, which is much higher than 0.05.\nE. Impact of Randomness in ProbDD\nFinding 3: Randomness has no significant impact on the\nperformance of ProbDD.\nIn ProbDD, elements with different probabilities are sorted\naccordingly, while elements with the same probability are\nrandomly shuffled. However, randomness alone intuitively\ndoes not ensure a higher probability of escaping local optima\nand the effect of this randomness on performance has not been\nthoroughly investigated.\nTo this end, we conduct an ablation study by removing such\nrandomness, creating a variant called ProbDD-no-random.\nWe evaluate this variant across all benchmarks. The results\nindicate that the randomness does not significantly impact\nperformance. Specifically, in terms of final size, execution\ntime, and query number, ProbDD-no-random achieves 236,\n2,069, and 1,238 compared to 235, 2,189, and 1,309 of\nProbDD, respectively. The p-values of 0.87, 0.15, and 0.10\nindicate that the differences are not significant.\nF. Bottleneck Overcome by ProbDD\nFinding 4: On tree-structured inputs, inefficient deletion\nattempts on complements and repeated attempts account for\nthe bottlenecks of ddmin, which are overcome by ProbDD.\nIn the study of ProbDD, the authors demonstrate that\nProbDD is more efficient than the baseline approach (ddmin)\nin tree-based reduction scenarios, where the inputs are parsed\ninto tree representations before reduction. Therefore, to un-\ncover the root cause of this superiority, we follow the same\napplication scenario and analyze the behavior of ProbDD in\nreducing the tree-structured inputs.\nTo further understand why ProbDD is more efficient than\nddmin, we conduct in-depth statistical analysis on the query\nnumber (number of deletion attempts). Intuitively, perfor-\nmance bottlenecks lie in those queries with low success rates,\nimpairing ddmin’s efficiency. Existing studies [17], [18] also\ndemonstrate the presence of queries with low success rates.\nTherefore, to qualitatively and quantitatively identify the exact\nbottlenecks impairing ddmin, we statistically analyze all the\nqueries in ddmin and categorize them into three types:\nTABLE IV: The final size, execution time and query number of ddmin, ProbDD and CDD on BMC and BMDBT. To address\nsignificant variations across benchmarks, the geometric mean rather than the arithmetic mean is employed, providing a smoother\nmeasure of the average.\nFinal size (#)\nExecution time (s)\nQuery number\nBenchmark\nOriginal size (#)\nddmin\nProbDD\nCDD\nddmin\nProbDD\nCDD\nddmin\nProbDD\nCDD\nLLVM-22382\n9,987\n350\n353\n350\n1,917\n1,163\n1,005\n11,388\n5,973\n5,262\nLLVM-22704\n184,444\n786\n764\n745\n27,924\n12,418\n11,371\n52,412\n15,425\n14,025\nLLVM-23309\n33,310\n1,316\n1,338\n1,265\n17,619\n9,991\n10,828\n55,968\n19,195\n17,953\nLLVM-23353\n30,196\n321\n336\n324\n3,117\n1,874\n1,400\n11,719\n5,757\n4,492\nLLVM-25900\n78,960\n941\n932\n937\n7,258\n3,683\n3,104\n35,740\n12,553\n12,817\nLLVM-26760\n209,577\n520\n503\n498\n13,123\n5,876\n5,210\n30,063\n9,261\n9,792\nLLVM-27137\n174,538\n972\n1,040\n966\n63,971\n22,208\n23,154\n122,516\n22,292\n20,46",
    "75,913\n147,035\n27,569\n61,032\nmkdir-5.2.1\n34,801\n8,625\n8,782\n8,418\n3,227\n2,428\n1,877\n11,969\n2,836\n2,099\nrm-8.4\n44,459\n8,507\n8,467\n8,461\n12,087\n5,008\n5,109\n33,171\n5,097\n5,057\nsort-8.16\n88,068\n14,893\n14,843\n15,834\n60,631\n61,739\n21,948\n119,150\n18,711\n7,914\ntar-1.14\n163,296\n20,411\n20,713\n20,592\n115,234\n95,765\n77,910\n200,394\n14,384\n12,095\nuniq-8.16\n63,861\n14,350\n14,262\n14,354\n21,672\n23,177\n19,124\n25,886\n4,228\n3,669\nBMDBT\nMean\n65,151\n15,080\n15,152\n15,235\n43,827\n28,505\n21,782\n72,140\n11,803\n10,686\nddmin\nProbDD\nCDD\n0\n200\n400\n600\n800\nQuery Number (thousand)\n11,927 / 239,348 \n≈ 4.98%\n9 / 490,896 \n< 0.01%\n222 / 170,884 \n≈ 0.13%\n11,696 / 273,275 \n≈ 4.28%\n12,251 / 274,144 \n≈ 4.47%\nOther\nComplement\nRevisit\nProbDD\nCDD\n(a) On BMC\nddmin\nProbDD\nCDD\n0\n200\n400\n600\n800\n1000\nQuery Number (thousand)\n8,437 / 136,340 \n≈ 6.19%\n855 / 699,436 \n≈ 0.12%\n1,048 / 223,416 \n≈ 0.47%\n13,878 / 160,824 \n≈ 8.63%\n11,358 / 171,453 \n≈ 6.62%\nOther\nComplement\nRevisit\nProbDD\nCDD\n(b) On BMDBT\nddmin\nProbDD\nCDD\n0\n5\n10\n15\n20\n25\n30\nQuery Number (thousand)\n1,485 / 12,407 \n≈ 11.97%\n830 / 13,928 \n≈ 5.96%\n2 / 2,991 \n≈ 0.07%\n2,176 / 17,775 \n≈ 12.24%\n2,258 / 18,653 \n≈ 12.11%\nOther\nComplement\nRevisit\nProbDD\nCDD\n(c) On BMXML\nFig. 2: Visualization of queries within ddmin, ProbDD and CDD. In ddmin, three types of queries are displayed via stacked\nbars, the height of which denotes the query number. Within each bar, the number of successful queries, total queries and the\ncorresponding success rate are annotated.\n1) Complement: Queries attempting to remove the comple-\nment of a subset. According to ddmin algorithm, given a\nsubset (smaller than half of the list L), it attempts to remove\neither the subset or its complement. However, evidence [18]\nshows that keeping a small s",
    "eletion attempts\non earlier subsets. Although the removal of one subset\nmay allow another subset to be removable, such repetitions\nrarely succeed and thus offer limited improvement for the\nreduction [17].\n3) Other: All other queries.\nIn addition to categorizing queries in ddmin into the above\ntypes, we also calculate the success rate of each type, aiming\nto reveal the bottlenecks of ddmin. Fig. 2 illustrates the\ndistribution of queries for all types within ddmin, as well as the\nquery number for ProbDD across all three benchmark suites.\nOn all benchmark suites, the number of successful queries\nin ddmin and ProbDD is remarkably similar, especially when\nTABLE V: The final size, execution time and query number of ddmin, ProbDD and CDD on BMXML. The last row shows the\noverall average across all three benchmark suites.\nFinal size (#)\nExecution time (s)\nQuery number\nBenchmark\nOriginal size (#)\nddmin\nProbDD\nCDD\nddmin\nProbDD\nCDD\nddmin\nProbDD\nCDD\nxml-071d221-1\n20,090\n10\n15\n20\n73\n114\n144\n29\n50\n69\nxml-071d221-2\n20,387\n13\n14\n20\n155\n146\n243\n60\n69\n117\nxml-1e9bc83-1\n20,327\n38\n49\n24\n491\n522\n284\n235\n236\n115\nxml-1e9bc83-2\n20,222\n79\n78\n76\n1,391\n814\n867\n725\n390\n384\nxml-1e9bc83-3\n20,219\n69\n70\n72\n1,313\n893\n889\n619\n416\n404\nxml-1e9bc83-4\n19,985\n156\n139\n143\n3,911\n1,939\n2,521\n1,943\n935\n1,173\nxml-1e9bc83-5\n20,579\n81\n73\n75\n1,355\n929\n882\n746\n485\n428\nxml-1e9bc83-6\n19,880\n127\n126\n124\n3,563\n1,852\n1,548\n1,907\n964\n749\nxml-1e9bc83-7\n20,297\n111\n114\n111\n3,419\n",
    "l-8ede045-7\n20,054\n106\n106\n106\n2,851\n1,686\n1,237\n1,447\n614\n634\nxml-8ede045-8\n20,177\n76\n78\n76\n1,397\n1,327\n1,060\n597\n449\n475\nxml-f053486-1\n20,030\n10\n10\n10\n101\n134\n76\n31\n41\n28\nBMXML\nMean\n20,190\n56\n56\n58\n972\n819\n821\n453\n314\n327\nAll\nMean\n31,989\n233\n235\n237\n2,999\n2,189\n2,102\n2,752\n1,309\n1,320\ncontrasted with the substantial difference in the total number\nof queries. Specifically, on BMC, ddmin achieves 222 + 9 +\n11, 927 = 12, 158 successful queries, closely matching the\n11,696 successful queries from ProbDD. Similarly, on BMDBT\nand BMXML, ddmin performs 1, 048 + 855 + 8, 437 = 10, 340\nand 2+830+1, 485 = 2, 317 successful queries, respectively,\nboth closely aligning with the 13,878 and 2,176 successful\nqueries achieved by ProbDD. Besides, ddmin always performs\nsignificantly more failed queries, resulting in a larger total\nquery number and thus a longer execution time, as previously\ndiscussed in § IV-D.\nOn all benchmark suites, a large portion of ddmin’s queries\nis categorized as Complement and Revisit; however, they both\nhave a notably low success rate. For instance, on BMC, out of\na total of 901,128 queries, Complement and Revisit account for\n490,896 (54.48%) and 170,884 (18.96%), respectively. Within\nsuch queries in Complement and Revisit, merely 9 (<0.01%)\nand 222 (0.13%) queries succeed, i.e., only a tiny portion of\nattempts successfully reduce elements. These success rates are\nfar less than those of queries within Other (4.98%), as well\nas those of ProbDD (4.28%). On the other benchmark suites,\na similar phenomenon is observed.\nQueries within Complement and Revisit categories consti-\ntute a large portion yet prove to be largely inefficient, wasting\na significant amount of time and resources. On the contrary,\nthose in Other achieve a much higher success rate, on par with\nthat of ProbDD, and are responsible for most of the successful\ndeletions. Therefore, we believe that these two categories,\nwhere queries are inefficient, are the main bottlenecks behind\nddmin’s low efficiency. However, these bottlenecks are absent\nin ProbDD, as it does not consider complements of subsets\nand previously tried subsets for deletion.\nG. 1-Minimality of ProbDD?\nFinding 5: Improving efficiency by avoiding ineffective\nattempts presents a trade-off by not ensuring 1-minimality,\nwhile such limitation can be mitigated by iteratively running\nthe reduction algorithm until a fixpoint is reached.\nAlthough ProbDD avoids Revisit queries to enhance ef-\nficiency, some reduction potentials may be missed, as the\ndeletion of a certain subset may enable a previously tried sub-\nset to become removable. Therefore, a limitation of ProbDD\nlies in that it increases efficiency by sacrificing 1-minimality.\nTo substantiate this limitation, we examine how frequently\nProbDD generates a list that is not 1-minimal, i.e., can be\nfurther reduced by removing a single element. For instance,\nstatistical analysis on BMC reveals that among 6,871 invo-\ncations of ProbDD, 76 of them fail to generate a 1-minimal\nresult, accounting for 1.1%. For these failed invocations, an\naverage of 1.49 elements (tree nodes) can be further removed\nvia single-element deletion.\nHowever, such limitation is not apparent across all bench-\nmark suites, as the results from ProbDD are not consistently\nlarger than those from ddmin. Our further investigation reveals\nthat these benchmarks are reduced on wrapper frameworks\nPicireny and Chisel. Both frameworks employ iterative loops\nto achieve a fixpoint, effectively reducing some elements\nmissed in the first iteration.\nV. IMPLICATIONS: A COUNTER-BASED MODEL\nBuilding on the aforementioned demystification of ProbDD,\nwe discover that probability can be optimized away, and subset\nsize can be pre-computed. Hence, we propose Counter-Based\nDelta Debugging (CDD), to reduce the complexity of both\nthe theory and implementation of ProbDD, and validate the\ncorrectness of our prior theoretical proofs.\nSubset size pre-calculation.\nBased on Lemma III.3 in\n§ III-B, the size for each round can be pre-calculated. There-\nfore, as shown at line 12 – line 15 in Algorithm 2, we utilize\nthe current round r and the initial probability p0 to determine\nthe subset size s. The size of the selected subset decreases\nas the round counter increases. This is intuitively reasonable\nsince, after a sufficient number of attempts on a large size have\nbeen made, it becomes more advantageous to gradually reduce\nthe subset size for future trials. Furthermore, this trend aligns\nwell with that of ProbDD, in which probabilities of elements\ngradually increase, resulting in a smaller subset size.\nMain workflow.\nThe simplified ProbDD is illustrated in\nAlgorithm 2, from line 1 to line 11. Before each round, the\nCDD pre-calculates the subset size on line 3 and then partitions\nL using this size on line 4. Then, similar to ddmin, it attempts\nto remove each subset on line 5 – line 8. The subset size\ncontinuously decreases until it reaches 1, meaning that each\nelement will be individually removed once.\nRevisiting the running example.\nReturning to Table III,\nunder the same conditions, CDD achieves the same results\nas ProbDD but without the need for probability calculations.\nAlgorithm 2: CDD (L, ψ)\nInput: L: a list of element to be reduced.\nInput: ψ : L →B: the property to be preserved by L.\nInput: p0: the initial probability given by the user.\nOutput: the minimized list that still exhibits the property ψ .\n1 r ←0 // The round number, initially 0.\n2 do\n// Compute subset size by round number\n3\ns ←ComputeSize (r, p0)\n/* Partition L into subsets with s elements. If it does\nnot divide evenly, leave a smaller remainder ",
    "he size of the subset to be used in the current\nround.\n// Calculate the estimated probability of round r\n13\npr ←p0 × 1.582r\n// Calculate corresponding subset size of round r\n14\nsr = arg maxs∈N+ s × (1 −pr)s\n15\nreturn sr\nThis is because both the probability and subset size s can be\ndirectly determined from the round number r.\nEvaluation.\nAs shown in Table IV and Table V, CDD\noutperforms ddmin w.r.t. efficiency, with 29.91% less time and\n52.04% fewer queries. Meanwhile, CDD performs comparably\nto ProbDD w.r.t. final size, execution time and query number,\nwith a p-value of 0.42, 0.29 and 0.70, respectively, indicating\ninsignificance between these two algorithms. CDD is expected\nto perform on par with ProbDD since it is designed to\nprovide further insight and simplify the intricate design of\nProbDD, rather than to surpass its capabilities. Furthermore, its\ncomparable performance to ProbDD further validates the non-\nnecessity of randomness and our assumption in Lemma III.1.\nBottleneck and 1-minimality.\nRevisiting the bottlenecks\npresented in Fig. 2, CDD possesses a query number and\nsuccess rate close to those of ProbDD, indicating that CDD\nalso overcomes the bottlenecks of ddmin. Additionally, similar\nto ProbDD, 1-minimality is absent in CDD, although iterations\nhelp mitigate this issue.\nFinding 6: CDD always achieves comparable performance\nto ProbDD, which further supports our previous findings,\nincluding the theoretical simplifications regarding size and\nprobability, analysis of randomness, bottlenecks, and 1-\nminimality.\nVI. LIMITATIONS AND THREATS TO VALIDITY\nIn this section, we discuss the limitations of CDD, and po-\ntential factors that may impair the validity of our experimental\nresults.\nA. Limitations\nAs discussed in § IV-G, compared to ddmin, neither\nProbDD nor CDD guarantees 1-minimality. This limitation\narises because after successfully removing a subset, ddmin\nrestarts the process from the first subset, whereas ProbDD and\nCDD continue from the next subset, skipping all the previously\ntried subsets. Therefore, although ProbDD and CDD complete\nthe reduction process more quickly, they may miss certain\nreduction opportunities and produce larger results than ddmin.\nHowever, reduction and debloating tools generally invoke\nthese reduction algorithms in iterative loops until a fix-point\nis reached, gradually refining the results and mitigating limi-\ntations, as mentioned in § IV-G. Table IV and Table V further\nsupport this point by showing that with multiple iterations,\nProbDD and CDD achieve significantly higher efficiency\ncompared to ddmin, while still producing results that are\ncomparable to ddmin w.r.t. effectiveness.\nB. Threats to validity\nFor internal validity, the main threat comes from the po-\ntential impact from the assumption, as discussed in § III-A1.\nSpecifically, without assuming that the number of elements in\nL is always divisible by the current subset size, we could\nnot further refine the mathematical model of ProbDD to\nachieve a simpler representation. However, such assumption\nmight impact the actual performance, potentially negating\nthe benefits of our simplification. To this end, we conduct\nextensive empirical experiments, demonstrating that CDD, the\nsimplified algorithm derived from this assumption, exhibits no\nsignificant difference from ProbDD.\nFor external validity, the threat lies in the generalizability\nof our findings across application scenarios. To mitigate this\nthreat, we perform experiments on 76 benchmarks, including\nC programs triggering real-world compiler bugs, XML inputs\ncrashing XML processing tools, and benchmarks from soft-\nware debloating tasks. These benchmarks have covered various\nuse scenarios of minimization algorithms.\nVII. RELATED WORK\nIn this section, we discuss related work of test input\nminimization around t",
    " minimization is an NP-complete\nproblem, in which achieving the global minimum is usually\ninfeasible. Therefore, existing approaches to improving effec-\ntiveness mainly aim to escape local minima by performing\nmore exhaustive searches. Since enumerating all possible sub-\nsets is infeasible, Vulcan [34] and C-Reduce [14] enumerate\nall combinations of elements within a small sliding window,\nand exhaustively attempt to delete each combination, resulting\nin smaller final program sizes. In contrast, ProbDD and CDD\ndo not exhibit clear actions targeted at breaking through local\noptima, suggesting they cannot achieve better effectiveness\nthan ddmin, as aligned with our evaluation in § IV.\nEfficiency.\nIf parallelism is not considered, the core of\nboosting efficiency is the enhanced capability to avoid rela-\ntively inefficient queries. For example, Hodovan and Kiss [18]\nproposed disregarding attempts to remove the complement of\nsubsets, the success rate of which is unacceptably low in some\nscenarios. Besides, Gharachorlu and Sumner [17] proposed\nOne Pass Delta Debugging (OPDD), which continues with the\nsubset next to the deleted one, rather than starting over from\nthe first subset. This optimization also avoids some redundant\nqueries in ddmin, reducing runtime by 65%. As revealed by\nour analysis, these two above-mentioned optimizations are\nimplicitly incorporated within ProbDD and CDD, and thereby\ncontributing to their higher efficiency than ddmin.\nUtilization of domain knowledge.\nThere is an inherent\ntrade-off between effectiveness and efficiency in test input\nminimization. For the same algorithm, achieving a better\nresult, i.e., a smaller local optimum, requires more queries\nto be spent on trial and error. However, employing do-\nmain knowledge [14], [35]–[37] can still improve the overall\nperformance. For instance, J-Reduce is both more effective\nand efficient than HDD in reducing Java programs, as it\nescapes more local optima by program transformations while\nsimultaneously avoiding more inefficient queries via semantic\nconstraints, leveraging the semantics of Java. Our analysis on\nProbDD indicates that the probabilities primarily function as\ncounters and do not utilize or effectively learn the domain\nknowledge of an input. Besides, the evaluation on CDD, a sim-\nplified algorithm without utilizing probability, demonstrates\nthat prioritizing elements via such probabilities does not yield\nsignificant benefits, thus validating our analysis.\nVIII. CONCLUSION\nThis paper conducts the first in-depth analysis of ProbDD,\nwhich is the state-of-the-art variant of ddmin, to further\ncomprehend and demystify its superior performance. With\ntheoretical analysis of the probabilistic model in ProbDD,\nwe reveal that probabilities essentially serve as monotonically\nincreasing counters, and propose CDD for simplification. Eval-\nuations on 76 benchmarks from test input minimization and\nsoftware debloating confirm that CDD performs on par with\nProbDD, substantiating our theoretical analysis. Furthermore,\nour examination on query success rate and randomness uncov-\ners that ProbDD’s superiority stems from skipping inefficient\nqueries. Finally, we discuss trade-offs in ddmin and ProbDD,\nproviding insights for future research and applications of test\ninput minimization algorithms.\nACKNOWLEDGMENTS\nWe thank all the anonymous reviewers in ICSE’25 for their\ninsightful feedback and comments. This research is partially\nsupported by the Natural Sciences and Engineering Research\nCouncil of Canada (NSERC) through the Discovery Grant, a\nproject under WHJIL, and CFI-JELF Project #40736.\nREFERENCES\n[1] A. Zeller and R. Hildebrandt, “Simplifying and isolating failure-inducing\ninput,” IEEE ",
    "oiding the familiar to speed up test\ncase reduction,” in 2018 IEEE International Conference on Software\nQuality, Reliability and Security (QRS).\nIEEE, 2018, pp. 426–437.\n[18] R. Hodován and Á. Kiss, “Practical improvements to the minimizing\ndelta debugging algorithm.” in ICSOFT-EA, 2016, pp. 241–248.\n[19] X. Zhou, Z. Xu, M. Zhang, Y. Tian, and C. Sun, “Wdd: Weighted\ndelta debugging,” in Proceedings of the IEEE/ACM 47th International\nConference on Software Engineering, 2025.\n[20] G. Wang. (2021) Probdd. Accessed: 2023-04-30. [Online]. Available:\nhttps://github.com/Amocy-Wang/ProbDD\n[21] M. Zhang, Z. Xu, Y. Tian, X. Cheng, and C. Sun, “Artifact for\n\"toward a better understanding of probabilistic delta debugging\",” 2024.\n[Online]. Available: https://zenodo.org/records/14425530\n[22] A. Christi, A. Groce, and R. Gopinath, “Resource adaptation via test-\nbased software minimization,” in 2017 IEEE 11th International Con-\nference on Self-Adaptive and Self-Organizing Systems (SASO).\nIEEE,\n2017, pp. 61–70.\n[23] A. Groce, M. A. Alipour, C. Zhang, Y. Chen, and J. Regehr, “Cause\nreduction for quick testing,” in 2014 IEEE Seventh International Con-\nference on Software Testing, Verification and Validation.\nIEEE, 20"
  ],
  21
]