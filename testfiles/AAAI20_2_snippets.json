[
  [
    "we survey diﬀerent studies on DL-based\ncode generation. In particular, we survey the following three types of research: (1) research on proposing\nnew techniques, (2) research on empirical evaluations, and (3) research on constructing datasets. Due to\nthe speciﬁc characteristics of code, it is necessary to invent specialized deep learning techniques to deal\nwith code. Therefore, we can see a number of inﬂuential deep learning techniques for code stemming\nfrom the area of code generation. Table 4 [77–107] highlights the main technical contributions in code\ngeneration. Since code completion is one of the most important code generation applications, we also\nsurvey deep learning-based code completion.\nThe primary contributions of the analyzed studies can be classiﬁed into nine distinct categories, high-\nlighting the diverse range of advancements and innovations within code generation.\n4.1\nEnhancing code structure information\nSix studies highlight the oversight of rich structural information in many code generators [106]. To ad-\ndress this concern, diverse approaches have been proposed for incorporating additional code structure\ninformation into their DL-based models, aiming to enhance the overall performance. Rabinovich et al. [77]\nintroduced abstract syntax networks, a DL-based model that incorporates supplementary structural in-\nformation from abstract syntax trees (ASTs). Their model utilizes an encoder-decoder BiLSTM with\nhierarchical attention, aiming to generate well-formed and executable Python code fra",
    "ti-mode-based\nPre-trained\nPython\n[88, 89]\nPre-trained\nGo, Java, JavaScript, PHP, Python, Ruby\n[90]\nDL\nSQL\n[91]\nCompilability\nPre-trained\nPython\n[92]\nPre-trained\nSQL, Vega-Lite, SMCalFlow\n[93]\nDL\nJava, Python\n[94]\nDual learning-based\nPre-trained\nJava\n[95]\nPre-trained\nPython, SQL\n[96]\nDL\nPython\n[97]\nSearch-based\nDL\nC++\n[98]\nPre-trained\nJava, Python\n[99]\nContext-aware\nDL\nJava\n[100, 101]\nPre-trained\nPython\n[102]\nPracticality\nDL\npython, SQL\n[103]\nDL\nJavascript\n[104]\nLong dependency problem\nDL\nPython\n[105–107]\n4.3\nMulti-mode based code generation\nThe studies in this category construct the code generators by taking into account multiple code artifacts\nin a comprehensive manner. Le et al. [89] noticed that most code generators overlook certain crucial yet\npotentially valuable code speciﬁcations, such as unit tests, which frequently lead to subpar performance\nwhen addressing intricate or unfamiliar coding tasks. They thus introduced a new generation procedure\nwith a critical sampling strategy that allow",
    " representations. Shen et al. [104] proposed a task augmentation technique that integrates domain\nknowledge into code generation models, making their model the ﬁrst domain-speciﬁc code generation\nsystem adopted in industrial development environments.\n4.9\nLong dependency\nMany deep learning-based code generators are trained using RNNs such as LSTM1), BiLSTM2), and\nGRU [108]. To overcome the long-term dependency problem, three studies introduce novel techniques\nto tackle this challenge. Sun et al. [106] proposed a novel tree-based neural architecture and applied the\nattention mechanism of transformers to alleviate the long-dependency problem. Xie et al. [107] utilized\nmutual distillation learning to train a code generator in order to avoid the occurrence of this problem.\n4.10\nCode completion\nTable 5 [109–114] shows code completion techniques published in the premier publication venues (i.e.,\nASE, ICSE, and FSE) from 2020 to 2023. Since 2020, there have been six papers focusing on the code\ncomp",
    "1–742\n104\nShen S, Zhu X, Dong Y, et al. Incorporating domain knowledge through task augmentation for front-end JavaScript code\ngeneration.\nIn: Proceedings of the 30th ACM Joint European Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, 2022. 1533–1543\n105\nSun Z, Zhu Q, Mou L, et al. A grammar-based structural CNN decoder for code generation. In: Proceedings of the AAAI\nConference on Artiﬁcial Intelligence, 2019. 7055–7062\n106\nSun Z, Zhu Q, Xiong Y, et al. TreeGen: a tree-based transformer architecture for code generation. In: Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 2020. 8984–8991\n107\nXie B, Su J, Ge Y, et al. Improving tree-structured decoder training for code generation via mutual learning. In: Proceedings\nof the AAAI Conference on Artiﬁcial Intelligence, 2021. 14121–14128\n108\nChung J, Gulcehre C, Cho K, et al. Empirical evaluation of gated recurrent neural networks on sequence modeling. 2014.\nArXiv:1412.3555\n109\nLiu F, Li G, Zha"
  ],
  106
]