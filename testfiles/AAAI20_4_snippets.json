[
  [
    "ensed to ACM.\nACM ISBN 978-1-4503-9156-6/22/04...$15.00\nhttps://doi.org/10.1145/3491101.3519665\non two different kinds of approaches: (1) program synthesis algo-\nrithms that search over a large program space defined by a domain-\nspecific language (DSL) [2, 7, 10, 12, 14, 19, 24, 25, 30, 31, 34, 43],\nand (2) deep learning models that are trained on a large amount\nof existing code and can generate new code given some forms of\nspecifications such as natural language descriptions or incomplete\ncode [5, 16, 17, 22, 38, 39, 48, 49]. Both kinds of approaches have\nclear drawbacks. On the one hand, existing program synthesis tech-\nniques are constrained to pre-defined DSLs and cannot scale to\ngeneral-purpose programming languages [15]. On the other hand,\nexisting generative models have a hard time learning sophisticated\nprogramming patterns from code corpora and often generate code\nwith syntactic or semantic errors [9, 29, 40]. The recent development\nof Large Language Models (LLM) such as GPT-3 [32] has opened up\nnew opportu",
    "ined DSL, making it less scalable\nto programs written in general-purpose programming languages\nsuch as Java or Python. Because general-purpose programming\nlanguages include much more language features and syntax rules\ncompared with DSLs and therefore define a much bigger program\nspace to search from [15].\nThe second trend is using machine learning, especially deep\nlearning. Advances in deep learning have shown promising re-\nsults on automatically generating code for real-world programming\ntasks [5, 16, 17, 22, 38, 39, 48, 49]. For instance, Kim et al. [21] de-\nveloped a transformer architecture that is aware of code structures\nusing abstract syntax trees. Alon et al. [1] introduced structural\nlanguage models that remove any restriction on the vocabulary or\nstructure— the main limitation of program synthesis techniques.\nKarampatsis and Sutton [20] similarly introduced open-vocabulary\nmodels that can generate code with an arbitrary number of tokens.\nThough these methods have shown promising results, they still\nsuffer ",
    "ah, Rastislav Bodík, and Kemal Ebcioğlu.\n2005. Programming by sketching for bit-streaming programs. In Proceedings of\nthe 2005 ACM SIGPLAN conference on Programming language design and imple-\nmentation. 281–294.\n[37] Simone Stumpf, Vidya Rajaram, Lida Li, Weng-Keen Wong, Margaret Burnett,\nThomas Dietterich, Erin Sullivan, and Jonathan Herlocker. 2009. Interacting\nmeaningfully with machine learning systems: Three experiments. International\njournal of human-computer studies 67, 8 (2009), 639–662.\n[38] Zeyu Sun, Qihao Zhu, Yingfei Xiong, Yican Sun, Lili Mou, and Lu Zhang. 2020.\nTreegen: A tree-based transformer architecture for code generation. In Proceed-\nings of the AAAI Conference on Artificial Intelligence, Vol. 34. 8984–8991.\n[39] Tabnine [n. d.]. Code Faster with AI Code Completions. https://www.tabnine.\ncom/. Accessed: 2022-1-8.\n[40] Michele Tufano, Cody Watson, Gabriele Bavota, Massimiliano Di Penta, Martin\nWhite, and Denys Poshyvanyk. 2019. An empirical study on learning bug-fixing\npatches in the wild via neural machine translation. ACM Transactions on Software\nEngin"
  ],
  38
]